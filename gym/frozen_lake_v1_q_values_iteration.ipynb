{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756da44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Frozen Lake v1 : Q value iteration examples\n",
    "################################################################\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters \n",
    "EPSILON_TRAIN = 0.5\n",
    "EPSILON_TEST = 0.0\n",
    "GAMMA = 0.999\n",
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "NUM_EPISODES = 200\n",
    "\n",
    "\n",
    "class GameAgent():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME, render_mode=\"rgb_array\")\n",
    "        self.STATE_DIM = self.env.observation_space.n\n",
    "        self.ACTION_DIM = self.env.action_space.n\n",
    "        self.epsilon = 0.5\n",
    "        self.GAMMA = GAMMA\n",
    "        self.lr = 0.1\n",
    "    \n",
    "    def initialize_game(self):\n",
    "        state, _ = self.env.reset() \n",
    "        return state\n",
    "    \n",
    "    def select_action(self, current_state=None):\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "        \n",
    "    def step_game(self, action):\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        return next_state, reward, terminated\n",
    "        \n",
    "    def run_episode(self, training=True):\n",
    "        current_state = self.initialize_game()\n",
    "        terminated = False\n",
    "        episode_reward = 0.0\n",
    "    \n",
    "        while not terminated:\n",
    "            # choose action\n",
    "            action = self.select_action(current_state)\n",
    "            \n",
    "            # take action\n",
    "            next_state, reward, terminated = self.step_game(action)\n",
    "\n",
    "            # prepare next step\n",
    "            current_state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        return episode_reward\n",
    "    \n",
    "    def run_epoch(self, training=True):\n",
    "        rewards = np.zeros(NUM_EPISODES)\n",
    "        for i in range(NUM_EPISODES):\n",
    "            rewards[i] = self.run_episode(training)\n",
    "        return rewards.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec58680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "optimal policy:\n",
      "[[0 3 3 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 3 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.6425000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ModelFreeAgent(GameAgent):\n",
    "    def __init__(self):\n",
    "        super(ModelFreeAgent, self).__init__()\n",
    "        self.q_table = np.zeros([self.STATE_DIM, self.ACTION_DIM])\n",
    "        self.values = np.zeros(self.STATE_DIM)\n",
    "        self.epsilon = 1.0\n",
    "        self.GAMMA = GAMMA\n",
    "        self.lr = 0.1\n",
    "        \n",
    "    def epsilon_greedy(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            _, action = np.unravel_index(np.argmax(self.q_table[state, :]), self.q_table.shape)\n",
    "        return action\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        return self.epsilon_greedy(state)\n",
    "\n",
    "    def update_q_table(self, current_state, action, next_state, reward, terminated):\n",
    "        q_max = self.q_table[next_state, :].max() \n",
    "        if not terminated:\n",
    "            self.q_table[current_state, action] += self.lr * (reward + self.GAMMA * q_max - self.q_table[current_state, action]) \n",
    "        else:\n",
    "            self.q_table[current_state, action] += self.lr * (reward - self.q_table[current_state, action]) \n",
    "        \n",
    "    def run_episode(self, training=True):\n",
    "        current_state = self.initialize_game()\n",
    "        terminated = False\n",
    "        episode_reward = 0.0\n",
    "        self.epsilon = 0.5 if training else 0.01\n",
    "        \n",
    "        while not terminated:\n",
    "            # choose action\n",
    "            action = self.select_action(current_state)\n",
    "            \n",
    "            # take action\n",
    "            next_state, reward, terminated = self.step_game(action)\n",
    "            \n",
    "            # update q_table\n",
    "            if training:\n",
    "                self.update_q_table(current_state, action, next_state, reward, terminated)\n",
    "\n",
    "            # prepare next step\n",
    "            current_state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        return episode_reward\n",
    "    \n",
    "    def show_policy(self):\n",
    "        policy = np.argmax(self.q_table, axis=1)\n",
    "        return policy\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def model_free_example():\n",
    "    agent = ModelFreeAgent()\n",
    "    for i in tqdm(range(10)):\n",
    "        agent.run_epoch()\n",
    "    print(\"\")\n",
    "    print(\"optimal policy:\")\n",
    "    print(agent.show_policy().reshape(4,4))\n",
    "    \n",
    "    rewards = np.zeros(10)\n",
    "    for i in tqdm(range(10)):\n",
    "        rewards[i] = agent.run_epoch(training=False)\n",
    "    print(\"\")\n",
    "    print(rewards.mean())\n",
    "    \n",
    "\n",
    "model_free_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3a3217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2842.97it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1315.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2982.81it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1233.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2370.16it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1102.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2982.73it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 531.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3106.63it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 2562.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2751.34it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 884.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3252.95it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 858.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3044.00it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 809.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2822.05it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 718.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2951.63it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 729.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3238.96it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 791.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2845.93it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 531.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2882.74it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 495.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 1754.81it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 518.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.735\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3018.33it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 592.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.78\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2941.28it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 502.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.76\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2785.51it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 460.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.86\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3332.61it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 561.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3234.02it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 492.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.82\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2807.38it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 447.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.81\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2661.69it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 520.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.815\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3224.18it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 520.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.81\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3147.98it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 482.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.82\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2765.97it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 552.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.83\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2956.89it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 497.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.77\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2829.53it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 511.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.835\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 2939.43it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 517.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.795\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3018.64it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 527.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3110.65it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 477.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.815\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 3058.78it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 525.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 0.855\n",
      "\n",
      "[[0 3 3 3]\n",
      " [0 0 0 0]\n",
      " [3 1 0 0]\n",
      " [0 2 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class ModelBasedAgent(GameAgent):\n",
    "    def __init__(self):\n",
    "        super(ModelBasedAgent, self).__init__()\n",
    "        self.q_table = np.zeros([self.STATE_DIM, self.ACTION_DIM])\n",
    "        self.transtions_model = np.zeros([self.STATE_DIM, self.ACTION_DIM, self.STATE_DIM])\n",
    "        self.rewards_model = np.zeros([self.STATE_DIM, self.ACTION_DIM, self.STATE_DIM])\n",
    "        self.lr = 0.1\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        if training:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            _, action = np.unravel_index(np.argmax(self.q_table[state, :]), self.q_table.shape)\n",
    "            return action\n",
    "\n",
    "    def q_values_iteration(self):\n",
    "        for current_state in range(self.STATE_DIM):\n",
    "            for action in range(self.ACTION_DIM):\n",
    "                total_bb = 0.0\n",
    "                total_transition = self.transtions_model[current_state, action, :].sum()\n",
    "                if total_transition == 0.0:\n",
    "                   continue      \n",
    "                probs = self.transtions_model[current_state, action, :] / total_transition\n",
    "                rewards = self.rewards_model[current_state, action, :] / total_transition\n",
    "                for next_state in range(self.STATE_DIM):\n",
    "                    reward = rewards[next_state]\n",
    "                    transition_prob = probs[next_state] \n",
    "                    q_max = self.q_table[next_state, :].max()\n",
    "                    bellman_backup = reward + self.GAMMA * q_max\n",
    "                    total_bb += transition_prob * bellman_backup\n",
    "                self.q_table[current_state, action] = total_bb\n",
    "    \n",
    "    def count_transition(self, current_state, action, next_state):\n",
    "        self.transtions_model[current_state, action, next_state] += 1\n",
    "    \n",
    "    def count_reward(self, current_state, action, next_state, reward):\n",
    "        self.rewards_model[current_state, action, next_state] += reward\n",
    "        \n",
    "    def run_episode(self, training=True):\n",
    "        current_state = self.initialize_game()\n",
    "        terminated = False\n",
    "        episode_reward = 0.0\n",
    "        \n",
    "        while not terminated:\n",
    "            # choose action\n",
    "            action = self.select_action(current_state, training)\n",
    "            \n",
    "            # take action\n",
    "            next_state, reward, terminated = self.step_game(action)\n",
    "            \n",
    "            # learn the model\n",
    "            self.count_transition(current_state, action, next_state)\n",
    "            self.count_reward(current_state, action, next_state, reward)\n",
    "            \n",
    "            # prepare next step\n",
    "            current_state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        return episode_reward\n",
    "        \n",
    "    def run_epoch(self, training=True):\n",
    "        rewards = np.zeros(NUM_EPISODES)\n",
    "        for i in range(NUM_EPISODES):\n",
    "            rewards[i] = self.run_episode(training)\n",
    "        return rewards.mean()\n",
    "    \n",
    "    def show_policy(self):\n",
    "        policy = np.argmax(self.q_table, axis=1)\n",
    "        return policy\n",
    "        \n",
    "    def reset():\n",
    "        pass\n",
    "\n",
    "\n",
    "def model_based_example():\n",
    "    agent = ModelBasedAgent()\n",
    "    iter_n = 0\n",
    "    while iter_n < 30:\n",
    "        # training\n",
    "        for _ in tqdm(range(200)):\n",
    "            agent.run_episode(training=True)\n",
    "        agent.q_values_iteration()\n",
    "    \n",
    "        # testing\n",
    "        rewards = np.zeros(200)\n",
    "        for i in tqdm(range(200)):\n",
    "            rewards[i] = agent.run_episode(training=False)\n",
    "        mean = rewards.mean()\n",
    "        print(f\"average reward: {mean}\")\n",
    "        print(\"\")\n",
    "        iter_n += 1\n",
    "    \n",
    "    print(agent.show_policy().reshape(4,4))\n",
    "\n",
    "model_based_example()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950acecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
